Weights and Biases
===================

++++++++++++++++++++++++++
What is Weights and Biases
++++++++++++++++++++++++++

.. image:: images/WB_logo.png
   :width: 150
   :align: center

For probably 99% of all neural networks, a hyperparameter search is needed. Things like the number of neurons,
then number of layers, the optimizer, the regularization etc. all need to be varied if you wish to get the best
performing model. This can quickly become cumbersome (given how quickly permuations grow). 

One way of hangling a hyperparameter search is to use a program called `Weights and Biases <https://wandb.ai/site>`_. In order to get started with W&B, follow this document.
This quickstart was written by Dr. Monique Shonde, so reach out to her if you have questions! 


++++++++++++++++++++++++++++++
Quickstart for W&B on Schooner
++++++++++++++++++++++++++++++
1. Create an account and install W&B
    a. a.	Sign up for free: `https://wandb.ai/site <https://wandb.ai/site>`_ and then login

2. Install the wandb into a Python 3 environment
    a. ``pip install wandb``

    or 

    b. ``mamba install -c conda-forge wandb``

3. From the command line, login to wandb
    a. You'll need your `API key <https://wandb.ai/authorize>`_ to login via the CLI
    b. ``wandb login``

4. Start a run and track hyperparameters
    a.	Initialize a W&B Run object with ``wandb.init()`` and pass a dictionary to the config parameter with key-value pairs of hyperparameter names and values:

    .. code-block:: python

        run = wandb.init(
            # Set the project where this run will be logged
            project="my-awesome-project",
            # Track hyperparameters and run metadata
            config={"learning_rate": 0.01,
                    "epochs": 10,})
    

    b.	Use the WandbMetricsLogger() callback to automatically track the loss and metrics

    .. code-block:: python

        # Modern lightweight callback
        # Tracks the loss and metrics
        # note: wandb callbacks can only be initialized after 
        #       wandb.init()
        metrics_callback = WandbMetricsLogger()

        # Train model
        history = model.fit(x_train, y_train, epochs=config['epochs'], 
                    callbacks=[metrics_callback])

    c.	Go to your `wandb home <https://wandb.ai/home>`_ to view the details of the experiment and watch run in real-time

    d.	You can find the specific urls for the unique runs in the error field generated by slurm jobs
    
Additional Resources:

AI2ES Coding Standards: Documentation Standards by Jay Rothenberger

    - `https://drive.google.com/file/d/1tAzeVyfXL1liif513ZVDFXbuPOm_HcdN/view <https://drive.google.com/file/d/1tAzeVyfXL1liif513ZVDFXbuPOm_HcdN/view>`_

Using wandb with Keras Tuners:

    - `Automate Hyperparameter Tuning Using Keras-Tuner and W&B | keras-tuner â€“ Weights & Biases (wandb.ai) <https://wandb.ai/arig23498/keras-tuner/reports/Automate-Hyperparameter-Tuning-Using-Keras-Tuner-and-W-B--Vmlldzo0MzQ1NzU>`_
